---
title: Learning Transductions and Alignments with RNN Seq2seq Models
abstract: 'The paper studies the capabilities of Recurrent-Neural-Network sequence
  to sequence (RNN seq2seq) models in learning four transduction tasks: identity,
  reversal, total reduplication, and quadratic copying. These transductions are traditionally
  well studied under finite state transducers and attributed with increasing complexity.
  We find that RNN seq2seq models are only able to approximate a mapping that fits
  the training or in-distribution data, instead of learning the underlying functions.
  Although attention makes learning more efficient and robust, it does not overcome
  the out-of-distribution generalization limitation. We establish a novel complexity
  hierarchy for learning the four tasks for attention-less RNN seq2seq models, which
  may be understood in terms of the complexity hierarchy of formal languages, instead
  of string transductions. RNN variants also play a role in the results. In particular,
  we show that Simple RNN seq2seq models cannot count the input length.  \\'
section: Regular papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23a
month: 0
tex_title: Learning Transductions and Alignments with RNN Seq2seq Models
firstpage: 223
lastpage: 249
page: 223-249
order: 223
cycles: false
bibtex_author: Wang, Zhengxiang
author:
- given: Zhengxiang
  family: Wang
date: 2023-07-10
address:
container-title: Proceedings of 16th edition of the International Conference on Grammatical
  Inference
volume: '217'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 10
pdf: https://proceedings.mlr.press/v217/wang23a/wang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
