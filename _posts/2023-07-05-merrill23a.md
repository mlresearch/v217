---
title: Formal languages and neural models for learning on sequences
abstract: The empirical success of deep learning in NLP and related fields motivates
  understanding the model of grammar implicit within neural networks on a theoretical
  level. In this tutorial, I will overview recent empirical and theoretical insights
  on the power of neural networks as formal language recognizers. We will cover the
  classical proof that infinite-precision RNNs are Turing-complete, formal analysis
  and experiments comparing the relative power of different finite-precision RNN architectures,
  and recent work characterizing transformers as language recognizers using circuits
  and logic. We may also cover applications of this work, including the extraction
  of discrete models from neural networks. Hopefully, the tutorial will synthesize
  different analysis frameworks and findings about neural networks into a coherent
  narrative, and provide a call to action for the ICGI community to engage with exciting
  open questions.
section: ICGI Angluin tutorial session
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: merrill23a
month: 0
tex_title: Formal languages and neural models for learning on sequences
firstpage: 5
lastpage: 5
page: 5-5
order: 5
cycles: false
bibtex_author: Merrill, William
author:
- given: William
  family: Merrill
date: 2023-07-05
address:
container-title: Proceedings of 16th edition of the International Conference on Grammatical
  Inference
volume: '217'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 5
pdf: https://proceedings.mlr.press/v217/merrill23a/merrill23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
